{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeVpdz99fs5h77IvSesTO3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soporteunah/bookllmraggemini/blob/main/llm_rag_gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm3sVxHKJfU3",
        "outputId": "9a036322-8f5a-41be-8ea8-6f59f2dc3cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.12/dist-packages (0.14.14)\n",
            "Requirement already satisfied: llama-index-llms-google-genai in /usr/local/lib/python3.12/dist-packages (0.8.7)\n",
            "Requirement already satisfied: llama-index-embeddings-google-genai in /usr/local/lib/python3.12/dist-packages (0.3.2)\n",
            "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.3)\n",
            "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.14 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.14.14)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.9.4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.6.18)\n",
            "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.6)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: google-genai<2,>=1.52.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-google-genai) (1.62.0)\n",
            "Requirement already satisfied: pillow>=10.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-google-genai) (11.3.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2,>=1.52.0->llama-index-llms-google-genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (2.47.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2,>=1.52.0->llama-index-llms-google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2,>=1.52.0->llama-index-llms-google-genai) (2.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2,>=1.52.0->llama-index-llms-google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2,>=1.52.0->llama-index-llms-google-genai) (9.1.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2,>=1.52.0->llama-index-llms-google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2,>=1.52.0->llama-index-llms-google-genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2,>=1.52.0->llama-index-llms-google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2,>=1.52.0->llama-index-llms-google-genai) (1.3.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (3.13.3)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (0.22.1)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (2.4.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (2025.3.0)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (2.14.2)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (3.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (2.0.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (4.5.1)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (6.0.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (82.0.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.14->llama-index) (2.0.46)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (4.67.3)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index) (1.17.3)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (2.17.0)\n",
            "Requirement already satisfied: llama-cloud==0.1.35 in /usr/local/lib/python3.12/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2026.1.4)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: pandas<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<7,>=6.1.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.7.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (2025.11.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (3.11)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.14->llama-index) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.14->llama-index) (3.1.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.8.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.14->llama-index) (0.4.2)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.54 in /usr/local/lib/python3.12/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.14->llama-index) (3.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.14->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.14->llama-index) (3.26.2)\n",
            "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15.0,>=0.14.14->llama-index) (26.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2,>=1.52.0->llama-index-llms-google-genai) (0.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: griffecli==2.0.0 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.14->llama-index) (2.0.0)\n",
            "Requirement already satisfied: griffelib==2.0.0 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.14->llama-index) (2.0.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffecli==2.0.0->griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.14->llama-index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.14->llama-index) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U llama-index llama-index-llms-google-genai llama-index-embeddings-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "from google.colab import userdata\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# --- Importaciones NUEVAS (Google GenAI) ---\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Obtener API Key\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "except:\n",
        "    # Si no usas los secretos de Colab, pega tu clave aqu√≠ abajo:\n",
        "    GOOGLE_API_KEY = \"TU_CLAVE_AQUI\"\n",
        "\n",
        "# --- Configuraci√≥n del Modelo (LLM) ---\n",
        "# Usamos GoogleGenAI en lugar de la clase Gemini antigua.\n",
        "# IMPORTANTE: Cambi√© \"2.5\" a \"1.5-pro\" porque el 2.5 no existe p√∫blicamente a√∫n.\n",
        "Settings.llm = GoogleGenAI(\n",
        "    model_name=\"models/gemini-2.5-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# --- Configuraci√≥n de Embeddings ---\n",
        "# Usamos GoogleGenAIEmbedding\n",
        "Settings.embed_model = GoogleGenAIEmbedding(\n",
        "    model_name=\"models/gemini-embedding-001\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "print(\"¬°Configuraci√≥n de Google GenAI completada con √©xito!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFHaveLOJ40F",
        "outputId": "9a045a7a-1299-495c-f9f8-62f2b8825d34"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬°Configuraci√≥n de Google GenAI completada con √©xito!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Descargar el archivo PDF\n",
        "!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# 2. Cargar el documento\n",
        "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()\n",
        "\n",
        "# 3. Dividir el texto en \"nodos\" (chunks de 1024 caracteres)\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "print(f\"Documento cargado. Se han creado {len(nodes)} nodos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5Q8PyK8KGVu",
        "outputId": "fe6f5e58-b0a5-4078-9cc0-042b9c1b12c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-16 20:16:12--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: ‚Äòmetagpt.pdf‚Äô\n",
            "\n",
            "metagpt.pdf         100%[===================>]  16.13M  38.0MB/s    in 0.4s    \n",
            "\n",
            "2026-02-16 20:16:13 (38.0 MB/s) - ‚Äòmetagpt.pdf‚Äô saved [16911937/16911937]\n",
            "\n",
            "Documento cargado. Se han creado 34 nodos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "# Importante: Al cambiar el modelo de embeddings, el √≠ndice se regenera.\n",
        "print(\"Creando √≠ndices con el nuevo modelo...\")\n",
        "\n",
        "# Crear √≠ndice de resumen\n",
        "summary_index = SummaryIndex(nodes)\n",
        "\n",
        "# Crear √≠ndice vectorial (Ahora s√≠ deber√≠a funcionar)\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "print(\"¬°√âXITO! √çndices creados correctamente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdDQ1TXeKM-B",
        "outputId": "0b0298a9-9be0-4755-ccb0-f9e7eefe1a53"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creando √≠ndices con el nuevo modelo...\n",
            "¬°√âXITO! √çndices creados correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "# 1. Motor de Resumen\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True\n",
        ")\n",
        "\n",
        "# 2. Motor Vectorial\n",
        "vector_query_engine = vector_index.as_query_engine()\n",
        "\n",
        "# 3. Empaquetar como Herramientas (Tools) con descripciones\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=\"Useful for summarization questions related to MetaGPT\"\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=\"Useful for retrieving specific context from the MetaGPT paper.\"\n",
        ")\n",
        "\n",
        "print(\"Herramientas configuradas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs9a4NRPKSl1",
        "outputId": "b9500c48-5584-4161-8b07-91e721b8c68b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Herramientas configuradas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "# Crear el Router\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True # Esto nos mostrar√° en pantalla qu√© decisi√≥n toma el modelo\n",
        ")\n",
        "\n",
        "print(\"Router Query Engine listo para usar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rodUUlatKX5F",
        "outputId": "add397e8-a136-45e4-8c1d-3b6a21f43b65"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Router Query Engine listo para usar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(\"\\n--- Respuesta ---\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWwDSlCWKcP_",
        "outputId": "9d23d840-69c3-4aec-8405-f621a351411f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for a summary of the document, and choice 1 explicitly states it is useful for summarization questions..\n",
            "\u001b[0m\n",
            "--- Respuesta ---\n",
            "MetaGPT is a meta-programming framework that uses Standardized Operating Procedures (SOPs) to enhance problem-solving in multi-agent systems using Large Language Models (LLMs). It assigns specialized roles to agents and uses an assembly line approach to break down complex tasks. The framework incorporates efficient human workflows into LLM-based multi-agent collaborations, allowing agents to verify intermediate results and reduce errors. MetaGPT achieves state-of-the-art performance on software engineering benchmarks by generating coherent solutions. It offers role definition, message sharing, and a novel executive feedback mechanism for debugging and code execution. The framework aims to introduce human practice into multi-agent frameworks and regulate LLM-based multi-agent systems.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"How do agents share information with other agents?\")\n",
        "print(\"\\n--- Respuesta ---\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrSG9BCZKiDB",
        "outputId": "5908292f-58e9-4cc4-acf5-4f9ead24f31c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: The question 'How do agents share information with other agents?' requires retrieving specific details about the MetaGPT framework's implementation. Choice (2) is designed for retrieving specific context from the MetaGPT paper, making it the more relevant option..\n",
            "\u001b[0m\n",
            "--- Respuesta ---\n",
            "Agents share information by publishing structured messages to a shared message pool where other agents can access them. Agents can also subscribe to specific information based on their role profiles, allowing them to receive only task-related information.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# 1. Definir funciones normales de Python\n",
        "def add(x: int, y: int) -> int:\n",
        "    \"\"\"Adds two integers together.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "def mystery(x: int, y: int) -> int:\n",
        "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
        "    return (x + y) * (x + y)\n",
        "\n",
        "# 2. Convertirlas en \"Herramientas\" para la IA\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "mystery_tool = FunctionTool.from_defaults(fn=mystery)\n",
        "\n",
        "# 3. Probar: Le pedimos a Gemini que use la funci√≥n \"mystery\"\n",
        "# Usamos predict_and_call, que hace que el LLM decida qu√© funci√≥n ejecutar y con qu√© n√∫meros.\n",
        "response = Settings.llm.predict_and_call(\n",
        "    [add_tool, mystery_tool],\n",
        "    \"Tell me the output of the mystery function on 2 and 9\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7bFg41lKkuL",
        "outputId": "1eb3b5ef-28c5-4a77-a5ce-837a33d34f15"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: mystery with args: {\"y\": 9, \"x\": 2}\n",
            "=== Function Output ===\n",
            "121\n",
            "121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from llama_index.core.vector_stores import FilterCondition, MetadataFilters\n",
        "\n",
        "# 1. Definir la funci√≥n de b√∫squeda con filtros\n",
        "def vector_query(query: str, page_numbers: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Perform a vector search over an index, filtering by specific page numbers.\n",
        "\n",
        "    Args:\n",
        "        query (str): The string query to be embedded.\n",
        "        page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to search all pages.\n",
        "    \"\"\"\n",
        "\n",
        "    # Crear los filtros de metadatos basados en los n√∫meros de p√°gina\n",
        "    metadata_dicts = [\n",
        "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "    ]\n",
        "\n",
        "    # Configurar el motor de consulta con esos filtros\n",
        "    query_engine = vector_index.as_query_engine(\n",
        "        similarity_top_k=2,\n",
        "        filters=MetadataFilters.from_dicts(\n",
        "            metadata_dicts,\n",
        "            condition=FilterCondition.OR\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Ejecutar la consulta y devolver la respuesta\n",
        "    response = query_engine.query(query)\n",
        "    return str(response)\n",
        "\n",
        "# 2. Convertir la funci√≥n en una Herramienta\n",
        "vector_query_tool = FunctionTool.from_defaults(\n",
        "    name=\"vector_tool\",\n",
        "    fn=vector_query\n",
        ")\n",
        "\n",
        "print(\"Herramienta de b√∫squeda por p√°gina creada.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yESFlcYMLRo9",
        "outputId": "6676bc25-d9d8-4bfe-e0cc-b653c5076623"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Herramienta de b√∫squeda por p√°gina creada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemini deber√≠a llamar a vector_query con page_numbers=['2']\n",
        "response = Settings.llm.predict_and_call(\n",
        "    [vector_query_tool],\n",
        "    \"What are the high-level results of MetaGPT as described on page 2?\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n--- Respuesta ---\")\n",
        "print(str(response))\n",
        "\n",
        "# Verificar que realmente us√≥ la p√°gina 2 revisando los metadatos (si la respuesta trajo nodos)\n",
        "# Nota: predict_and_call a veces devuelve solo texto, depende de la versi√≥n."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1YFE5qxLegp",
        "outputId": "b5b618f4-6d2e-48db-9105-3e76f4845dd5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
            "=== Function Output ===\n",
            "MetaGPT achieves a new state-of-the-art with 85.9% and 87.7% in Pass@1 in code generation benchmarks. It also achieves a 100% task completion rate.\n",
            "\n",
            "\n",
            "--- Respuesta ---\n",
            "MetaGPT achieves a new state-of-the-art with 85.9% and 87.7% in Pass@1 in code generation benchmarks. It also achieves a 100% task completion rate.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "# 1. Asegurarnos de tener el summary_tool listo (como en la lecci√≥n 1)\n",
        "summary_index = SummaryIndex(nodes)\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True\n",
        ")\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    name=\"summary_tool\",\n",
        "    query_engine=summary_query_engine,\n",
        "    description=\"Useful if you want to get a summary of MetaGPT\"\n",
        ")\n",
        "\n",
        "# 2. Prueba A: Pregunta Espec√≠fica con p√°gina (Deber√≠a usar vector_tool)\n",
        "print(\"--- PRUEBA 1: Pregunta espec√≠fica (P√°gina 8) ---\")\n",
        "response = Settings.llm.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\",\n",
        "    verbose=True\n",
        ")\n",
        "print(str(response))\n",
        "\n",
        "# 3. Prueba B: Pregunta General (Deber√≠a usar summary_tool)\n",
        "print(\"\\n--- PRUEBA 2: Resumen general ---\")\n",
        "response = Settings.llm.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What is a summary of the paper?\",\n",
        "    verbose=True\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOU0ZHY1LvwC",
        "outputId": "27a5a9e3-92bd-420d-822f-036542970b8c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- PRUEBA 1: Pregunta espec√≠fica (P√°gina 8) ---\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool with args: {\"page_numbers\": [\"8\"], \"query\": \"MetaGPT comparisons with ChatDev\"}\n",
            "=== Function Output ===\n",
            "MetaGPT surpasses ChatDev on the SoftwareDev dataset across almost all metrics. It achieves an executability score of 3.75, which is close to flawless, and requires less time (503 seconds). MetaGPT also outperforms ChatDev in code statistics and human revision cost. While MetaGPT uses more tokens (24,613 or 31,255 versus 19,292), it needs fewer tokens per line of code (126.5/124.3 versus 248.9).\n",
            "\n",
            "MetaGPT surpasses ChatDev on the SoftwareDev dataset across almost all metrics. It achieves an executability score of 3.75, which is close to flawless, and requires less time (503 seconds). MetaGPT also outperforms ChatDev in code statistics and human revision cost. While MetaGPT uses more tokens (24,613 or 31,255 versus 19,292), it needs fewer tokens per line of code (126.5/124.3 versus 248.9).\n",
            "\n",
            "\n",
            "--- PRUEBA 2: Resumen general ---\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool with args: {\"input\": \"MetaGPT\"}\n",
            "=== Function Output ===\n",
            "MetaGPT is a meta-programming framework designed for multi-agent collaboration using large language models. It incorporates standardized operating procedures into prompt sequences, allowing agents with domain expertise to verify intermediate results and reduce errors. It uses an assembly line paradigm to assign roles to agents, breaking down complex tasks into subtasks. It also features a communication protocol that enhances role communication efficiency, using structured communication interfaces and a publish-subscribe mechanism. Additionally, MetaGPT has an executable feedback mechanism for self-correction to improve code generation quality during run-time.\n",
            "\n",
            "MetaGPT is a meta-programming framework designed for multi-agent collaboration using large language models. It incorporates standardized operating procedures into prompt sequences, allowing agents with domain expertise to verify intermediate results and reduce errors. It uses an assembly line paradigm to assign roles to agents, breaking down complex tasks into subtasks. It also features a communication protocol that enhances role communication efficiency, using structured communication interfaces and a publish-subscribe mechanism. Additionally, MetaGPT has an executable feedback mechanism for self-correction to improve code generation quality during run-time.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADAPTAR RAG CON GEMINI PRO\n"
      ],
      "metadata": {
        "id": "bbubORyMGu9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Importamos la clase NUEVA que aparece en tu captura (L√≠nea 2)\n",
        "from llama_index.core.agent import FunctionAgent\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# 2. Aseguramos las herramientas (como siempre)\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_index.as_query_engine(),\n",
        "    description=\"Useful for retrieving specific context from the MetaGPT paper.\",\n",
        "    name=\"vector_tool\"\n",
        ")\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_index.as_query_engine(response_mode=\"tree_summarize\"),\n",
        "    description=\"Useful for summarization questions related to MetaGPT\",\n",
        "    name=\"summary_tool\"\n",
        ")\n",
        "\n",
        "# 3. Creamos el Agente de la forma MODERNA\n",
        "# Nota: Ya no se usa .from_tools(), se usa el constructor directo.\n",
        "# Tampoco necesitamos 'Worker' ni 'Runner', el FunctionAgent ya lo hace todo.\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    llm=Settings.llm,\n",
        "    tools=[vector_tool, summary_tool],\n",
        "    system_prompt=\"Eres un asistente √∫til capaz de usar herramientas para responder preguntas sobre documentos.\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ ¬°Agente Moderno (v0.13) creado con √©xito!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvR5RDHNG9wz",
        "outputId": "8f36d645-b81c-44e2-eb6f-042e6250e2eb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ¬°Agente Moderno (v0.13) creado con √©xito!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"ü§ñ El Agente (Workflow) est√° pensando...\")\n",
        "\n",
        "# EN LA VERSI√ìN NUEVA (v0.13+):\n",
        "# 1. No existe .chat(), se usa .run()\n",
        "# 2. Es as√≠ncrono, por eso usamos 'await'\n",
        "# 3. No devuelve un objeto 'Response' cl√°sico, devuelve un evento de salida.\n",
        "\n",
        "result = await agent.run(\n",
        "    \"Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- Respuesta Final ---\")\n",
        "print(str(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmobm7fEL-xS",
        "outputId": "c594be5a-5eec-45e4-8216-ed6589138f2d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ El Agente (Workflow) est√° pensando...\n",
            "\n",
            "--- Respuesta Final ---\n",
            "MetaGPT employs five distinct roles within its software company structure: Product Manager, Architect, Project Manager, Engineer, and QA Engineer. These agents communicate by producing structured outputs such as documents and diagrams, utilizing a shared message pool for publishing and accessing information. Agents subscribe to pertinent information based on their roles, which helps them avoid information overload.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "\n",
        "print(\"üß† INICIO DE LA PRUEBA CON MEMORIA EXPL√çCITA\\n\")\n",
        "\n",
        "# 1. Creamos el \"Diario\" (Historial vac√≠o al principio)\n",
        "chat_history = []\n",
        "\n",
        "# --- TURNO 1 ---\n",
        "pregunta1 = \"What are the 5 agent roles defined in MetaGPT?\"\n",
        "print(f\"üë§ Usuario: {pregunta1}\")\n",
        "\n",
        "# AQUI ESTA EL TRUCO: Le pasamos el 'chat_history' al agente\n",
        "response1 = await agent.run(pregunta1, chat_history=chat_history)\n",
        "print(f\"ü§ñ Agente: {response1}\\n\")\n",
        "\n",
        "# ACTUALIZAMOS EL DIARIO MANUALMENTE\n",
        "# Guardamos lo que dijo el usuario y lo que respondi√≥ el agente\n",
        "chat_history.append(ChatMessage(role=MessageRole.USER, content=pregunta1))\n",
        "chat_history.append(ChatMessage(role=MessageRole.ASSISTANT, content=str(response1)))\n",
        "\n",
        "print(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "# --- TURNO 2 (La prueba de fuego) ---\n",
        "pregunta2 = \"Which of them is responsible for writing the code?\"\n",
        "print(f\"üë§ Usuario: {pregunta2}\")\n",
        "\n",
        "# Volvemos a llamar al agente, pero ahora 'chat_history' YA TIENE DATOS\n",
        "response2 = await agent.run(pregunta2, chat_history=chat_history)\n",
        "print(f\"ü§ñ Agente: {response2}\\n\")\n",
        "\n",
        "# (Opcional) Actualizamos el diario de nuevo por si queremos seguir hablando\n",
        "chat_history.append(ChatMessage(role=MessageRole.USER, content=pregunta2))\n",
        "chat_history.append(ChatMessage(role=MessageRole.ASSISTANT, content=str(response2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYJ3BzC9PILE",
        "outputId": "806fe905-c73e-4754-9308-8c3bac56dfb5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† INICIO DE LA PRUEBA CON MEMORIA EXPL√çCITA\n",
            "\n",
            "üë§ Usuario: What are the 5 agent roles defined in MetaGPT?\n",
            "ü§ñ Agente: MetaGPT defines five roles in its software company: Product Manager, Architect, Project Manager, Engineer, and QA Engineer.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üë§ Usuario: Which of them is responsible for writing the code?\n",
            "ü§ñ Agente: The Engineer role is responsible for writing the code in MetaGPT.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KzAKx1PRqA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " (Agente sobre 11 documentos)."
      ],
      "metadata": {
        "id": "YtyUMs7SRqjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Lista de URLs y nombres de archivo\n",
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yv6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"loftq.pdf\",\n",
        "    \"swebench.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "    \"zipformer.pdf\",\n",
        "    \"values.pdf\",\n",
        "    \"finetune_fair_diffusion.pdf\",\n",
        "    \"knowledge_card.pdf\",\n",
        "    \"metra.pdf\",\n",
        "    \"vr_mcl.pdf\"\n",
        "]\n",
        "\n",
        "# Descargar los archivos\n",
        "import requests\n",
        "import os\n",
        "\n",
        "print(\"‚¨áÔ∏è Descargando 11 papers cient√≠ficos... (Esto puede tardar unos segundos)\")\n",
        "for url, paper in zip(urls, papers):\n",
        "    if not os.path.exists(paper):\n",
        "        !wget \"{url}\" -O \"{paper}\" -q\n",
        "        print(f\"‚úÖ Descargado: {paper}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Ya existe: {paper}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaFvondmRuMP",
        "outputId": "5434b60b-edb3-4f15-c148-e19d486d0c36"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Descargando 11 papers cient√≠ficos... (Esto puede tardar unos segundos)\n",
            "‚ö†Ô∏è Ya existe: metagpt.pdf\n",
            "‚úÖ Descargado: longlora.pdf\n",
            "‚úÖ Descargado: loftq.pdf\n",
            "‚úÖ Descargado: swebench.pdf\n",
            "‚úÖ Descargado: selfrag.pdf\n",
            "‚úÖ Descargado: zipformer.pdf\n",
            "‚úÖ Descargado: values.pdf\n",
            "‚úÖ Descargado: finetune_fair_diffusion.pdf\n",
            "‚úÖ Descargado: knowledge_card.pdf\n",
            "‚úÖ Descargado: metra.pdf\n",
            "‚úÖ Descargado: vr_mcl.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "def get_doc_tools(file_path: str, name: str):\n",
        "    \"\"\"Lee un PDF y devuelve una herramienta de vector y una de resumen.\"\"\"\n",
        "\n",
        "    # 1. Leer documento\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "\n",
        "    # 2. Crear √≠ndices (Gemini Embeddings ya est√° configurado en Settings)\n",
        "    vector_index = VectorStoreIndex.from_documents(documents)\n",
        "    summary_index = SummaryIndex.from_documents(documents)\n",
        "\n",
        "    # 3. Crear Motores\n",
        "    vector_query_engine = vector_index.as_query_engine()\n",
        "    summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\")\n",
        "\n",
        "    # 4. Crear Herramientas con descripciones din√°micas\n",
        "    vector_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=vector_query_engine,\n",
        "        description=f\"Useful for retrieving specific context from the {name} paper.\",\n",
        "        name=f\"vector_tool_{name}\"\n",
        "    )\n",
        "\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=summary_query_engine,\n",
        "        description=f\"Useful for summarization questions related to {name}\",\n",
        "        name=f\"summary_tool_{name}\"\n",
        "    )\n",
        "\n",
        "    return vector_tool, summary_tool"
      ],
      "metadata": {
        "id": "eLNke7LcRzWY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex, Document\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# --- 1. Funci√≥n de Limpieza (Igual que antes) ---\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Elimina caracteres extra√±os que rompen el encoder UTF-8\"\"\"\n",
        "    if not text: return \"\"\n",
        "    return text.encode('utf-8', 'ignore').decode('utf-8')\n",
        "\n",
        "# --- 2. Funci√≥n get_doc_tools CORREGIDA ---\n",
        "def get_doc_tools(file_path: str, name: str):\n",
        "\n",
        "    # Leer documento\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "\n",
        "    cleaned_documents = []\n",
        "\n",
        "    # CORRECCI√ìN: En lugar de modificar doc.text, creamos un NUEVO Documento\n",
        "    for doc in documents:\n",
        "        safe_text = clean_text(doc.text)\n",
        "        # Creamos una copia limpia manteniendo los metadatos\n",
        "        new_doc = Document(text=safe_text, metadata=doc.metadata)\n",
        "        cleaned_documents.append(new_doc)\n",
        "\n",
        "    # Crear √≠ndices usando los documentos LIMPIOS\n",
        "    vector_index = VectorStoreIndex.from_documents(cleaned_documents)\n",
        "    summary_index = SummaryIndex.from_documents(cleaned_documents)\n",
        "\n",
        "    # Crear Motores\n",
        "    vector_query_engine = vector_index.as_query_engine()\n",
        "    summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\")\n",
        "\n",
        "    # Crear Herramientas\n",
        "    vector_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=vector_query_engine,\n",
        "        description=f\"Useful for retrieving specific context from the {name} paper.\",\n",
        "        name=f\"vector_tool_{name}\"\n",
        "    )\n",
        "\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=summary_query_engine,\n",
        "        description=f\"Useful for summarization questions related to {name}\",\n",
        "        name=f\"summary_tool_{name}\"\n",
        "    )\n",
        "\n",
        "    return vector_tool, summary_tool\n",
        "\n",
        "# --- 3. Ejecutar el Bucle ---\n",
        "paper_to_tools_dict = {}\n",
        "\n",
        "print(\"‚öôÔ∏è Procesando documentos (Intento final)...\")\n",
        "\n",
        "for paper in papers:\n",
        "    # Verificaci√≥n extra: Si el archivo est√° vac√≠o (como values.pdf), lo saltamos\n",
        "    if not os.path.exists(paper) or os.path.getsize(paper) == 0:\n",
        "        print(f\"‚ö†Ô∏è Saltando {paper} (Archivo vac√≠o o no existe)\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing: {paper}\")\n",
        "    try:\n",
        "        stem_name = Path(paper).stem\n",
        "        # Limpiamos el nombre para que sea v√°lido como nombre de herramienta (sin puntos ni espacios)\n",
        "        clean_stem_name = stem_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
        "\n",
        "        vector_tool, summary_tool = get_doc_tools(paper, clean_stem_name)\n",
        "        paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error cr√≠tico en {paper}: {e}\")\n",
        "\n",
        "# Aplanar la lista\n",
        "all_tools = [t for paper in paper_to_tools_dict for t in paper_to_tools_dict[paper]]\n",
        "\n",
        "print(f\"\\nüéâ ¬°√âxito Total! Se han creado {len(all_tools)} herramientas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkrUESBnS2NM",
        "outputId": "d5e3f26c-b704-4d10-e151-345ce31bcc55"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Procesando documentos (Intento final)...\n",
            "Processing: metagpt.pdf\n",
            "Processing: longlora.pdf\n",
            "Processing: loftq.pdf\n",
            "Processing: swebench.pdf\n",
            "Processing: selfrag.pdf\n",
            "Processing: zipformer.pdf\n",
            "‚ö†Ô∏è Saltando values.pdf (Archivo vac√≠o o no existe)\n",
            "Processing: finetune_fair_diffusion.pdf\n",
            "Processing: knowledge_card.pdf\n",
            "Processing: metra.pdf\n",
            "Processing: vr_mcl.pdf\n",
            "\n",
            "üéâ ¬°√âxito Total! Se han creado 20 herramientas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import FunctionAgent\n",
        "from llama_index.core import Settings\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"ü§ñ Creando el Agente Investigador con acceso a 10 Papers (20 Herramientas)...\")\n",
        "\n",
        "# Creamos el agente moderno (v0.13)\n",
        "# Le damos la lista completa 'all_tools'\n",
        "agent_super = FunctionAgent(\n",
        "    llm=Settings.llm,\n",
        "    tools=all_tools,\n",
        "    system_prompt=\"\"\"\n",
        "    Eres un investigador de IA experto. Tienes acceso a una biblioteca de papers cient√≠ficos.\n",
        "\n",
        "    Tus instrucciones son:\n",
        "    1. Para buscar detalles espec√≠ficos (n√∫meros, resultados), usa las herramientas 'vector_tool'.\n",
        "    2. Para entender de qu√© trata un paper, usa las herramientas 'summary_tool'.\n",
        "    3. Si te piden comparar, busca la informaci√≥n en los papers relevantes y contrasta los resultados.\n",
        "    \"\"\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ ¬°Agente listo y armado con conocimiento!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot1XqgphToJj",
        "outputId": "4fc99f8a-f63d-4930-cc58-49c42380953c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Creando el Agente Investigador con acceso a 10 Papers (20 Herramientas)...\n",
            "‚úÖ ¬°Agente listo y armado con conocimiento!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üß† Ejecutando consulta compleja entre m√∫ltiples documentos...\")\n",
        "\n",
        "pregunta_final = \"\"\"\n",
        "Tell me about the evaluation dataset used in MetaGPT\n",
        "and compare it against the one used in SWE-Bench.\n",
        "Which one is more focused on software engineering tasks?\n",
        "\"\"\"\n",
        "\n",
        "# Usamos .run() con await\n",
        "resultado = await agent_super.run(pregunta_final)\n",
        "\n",
        "print(\"\\n--- üèÜ RESPUESTA FINAL ---\")\n",
        "print(str(resultado))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk2pBHGzTqKe",
        "outputId": "ef176d3c-0fd0-456e-a432-7e9801f11f10"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Ejecutando consulta compleja entre m√∫ltiples documentos...\n",
            "\n",
            "--- üèÜ RESPUESTA FINAL ---\n",
            "MetaGPT uses HumanEval, MBPP, and SoftwareDev for evaluation. SWE-bench uses a dataset of software engineering tasks where the evaluation requires at least one test to change from fail to pass.\n",
            "\n",
            "SWE-Bench is more focused on software engineering tasks.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADPTANDO A GRADIO PARA QUE SEA MAS AMIGABLE"
      ],
      "metadata": {
        "id": "pN8ZJlMiU1h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufyyg_xUU6AK",
        "outputId": "a90062e4-bc77-48aa-c7fd-3ec33ba08642"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.128.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (26.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.22)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.15.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.4.2)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.3)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.21.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1qwnw0iVGvb",
        "outputId": "46add79c-93c4-47ba-d6de-fd85879054f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-embeddings-huggingface in /usr/local/lib/python3.12/dist-packages (0.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.0)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-huggingface) (0.14.14)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-huggingface) (5.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.3)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.15.0)\n",
            "\u001b[33mWARNING: huggingface-hub 1.4.0 does not provide the extra 'inference'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.13.3)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.22.1)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.4.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.2.0)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.14.2)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.6.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (4.5.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.12.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (82.0.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.46)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (9.1.3)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.12.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.17.3)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (5.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.16.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.1.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.3.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.26.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Requirement already satisfied: griffecli==2.0.0 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.0)\n",
            "Requirement already satisfied: griffelib==2.0.0 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffecli==2.0.0->griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import nest_asyncio\n",
        "from pathlib import Path\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex, Document, Settings\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.agent import FunctionAgent\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "# CAMBIO: Usaremos HuggingFace para los embeddings (m√°s estable)\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "\n",
        "# 1. PARCHE ASYNCIO\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 2. API KEY (Solo para Gemini Chat, ya no para embeddings)\n",
        "try:\n",
        "    MY_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "except:\n",
        "    MY_API_KEY = \"TU_CLAVE_AQUI\"\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = MY_API_KEY\n",
        "\n",
        "# 3. CONFIGURACI√ìN DE MODELOS (LA SOLUCI√ìN)\n",
        "try:\n",
        "    # A) LLM: Usamos Gemini 1.5 Pro (Este s√≠ funciona)\n",
        "    Settings.llm = GoogleGenAI(\n",
        "        model_name=\"models/gemini-1.5-pro\",\n",
        "        temperature=0,\n",
        "        api_key=MY_API_KEY\n",
        "    )\n",
        "\n",
        "    # B) EMBEDDINGS: Usamos un modelo LOCAL multiling√ºe (Espa√±ol/Ingl√©s)\n",
        "    # Esto elimina el error 404 para siempre.\n",
        "    print(\"‚¨áÔ∏è Descargando modelo de embeddings local (esto toma unos segundos una sola vez)...\")\n",
        "    Settings.embed_model = HuggingFaceEmbedding(\n",
        "        model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "    )\n",
        "    print(\"‚úÖ Modelos configurados: Gemini 1.5 Pro + HuggingFace Embeddings.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error fatal configurando modelos: {e}\")\n",
        "\n",
        "global_agent = None\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    if not text: return \"\"\n",
        "    return text.encode('utf-8', 'ignore').decode('utf-8')\n",
        "\n",
        "# --- 4. PROCESAMIENTO ---\n",
        "def process_and_create_agent(files):\n",
        "    global global_agent\n",
        "\n",
        "    if not files:\n",
        "        return \"‚ö†Ô∏è Por favor, sube archivos primero.\"\n",
        "\n",
        "    all_tools = []\n",
        "    processed_names = []\n",
        "    errors = []\n",
        "\n",
        "    print(f\"üìÇ Procesando {len(files)} archivos de Gobierno...\")\n",
        "\n",
        "    for file_path in files:\n",
        "        try:\n",
        "            filename = Path(file_path).name\n",
        "            # Limpieza de nombre\n",
        "            stem_name = Path(file_path).stem.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
        "\n",
        "            # 1. Cargar\n",
        "            docs = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "\n",
        "            # 2. Limpiar Texto\n",
        "            cleaned_docs = [Document(text=clean_text(d.text), metadata=d.metadata) for d in docs]\n",
        "\n",
        "            # 3. Indexar (Ahora usa HuggingFace, no fallar√°)\n",
        "            vector_index = VectorStoreIndex.from_documents(cleaned_docs)\n",
        "            summary_index = SummaryIndex.from_documents(cleaned_docs)\n",
        "\n",
        "            # 4. Crear Tools\n",
        "            vector_tool = QueryEngineTool.from_defaults(\n",
        "                query_engine=vector_index.as_query_engine(),\n",
        "                description=f\"Useful for retrieving specific context from document: {filename}\",\n",
        "                name=f\"vector_tool_{stem_name[:50]}\" # Recortamos nombres muy largos\n",
        "            )\n",
        "            summary_tool = QueryEngineTool.from_defaults(\n",
        "                query_engine=summary_index.as_query_engine(response_mode=\"tree_summarize\"),\n",
        "                description=f\"Useful for summarization of document: {filename}\",\n",
        "                name=f\"summary_tool_{stem_name[:50]}\"\n",
        "            )\n",
        "\n",
        "            all_tools.extend([vector_tool, summary_tool])\n",
        "            processed_names.append(filename)\n",
        "            print(f\"‚úÖ {filename} OK\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en {filename}: {e}\")\n",
        "            errors.append(f\"{filename}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if not all_tools:\n",
        "        return f\"‚ùå Error total. Detalles: {errors}\"\n",
        "\n",
        "    # Crear el Agente\n",
        "    try:\n",
        "        global_agent = FunctionAgent(\n",
        "            llm=Settings.llm,\n",
        "            tools=all_tools,\n",
        "            system_prompt=\"Eres un experto en gesti√≥n p√∫blica y an√°lisis de documentos. Responde siempre en espa√±ol.\",\n",
        "            verbose=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"Error creando agente: {e}\"\n",
        "\n",
        "    return f\"‚úÖ ¬°Listo! {len(processed_names)} documentos activos.\"\n",
        "\n",
        "# --- 5. CHAT ---\n",
        "async def chat_logic(message, history):\n",
        "    global global_agent\n",
        "    if global_agent is None:\n",
        "        return \"‚ö†Ô∏è Primero procesa los archivos.\"\n",
        "\n",
        "    chat_history_objs = []\n",
        "    for human_msg, ai_msg in history:\n",
        "        chat_history_objs.append(ChatMessage(role=MessageRole.USER, content=human_msg))\n",
        "        if ai_msg:\n",
        "            chat_history_objs.append(ChatMessage(role=MessageRole.ASSISTANT, content=str(ai_msg)))\n",
        "\n",
        "    try:\n",
        "        response = await global_agent.run(message, chat_history=chat_history_objs)\n",
        "        return str(response)\n",
        "    except Exception as e:\n",
        "        return f\"Error en el chat: {str(e)}\"\n",
        "\n",
        "# --- INTERFAZ ---\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# üáµüá™ Analista de Planes de Gobierno (Gemini + Local Embeddings)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            file_input = gr.File(label=\"üìÇ Sube Planes de Gobierno (PDF)\", file_count=\"multiple\", file_types=[\".pdf\"])\n",
        "            process_btn = gr.Button(\"‚öôÔ∏è Procesar Documentos\", variant=\"primary\")\n",
        "            status_box = gr.Textbox(label=\"Estado\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.ChatInterface(\n",
        "                fn=chat_logic,\n",
        "                chatbot=gr.Chatbot(height=600),\n",
        "                textbox=gr.Textbox(placeholder=\"Ej: Compara el plan de Electroperu con el del Ministerio de Defensa...\", container=False, scale=7),\n",
        "                title=\"üí¨ Chat\"\n",
        "            )\n",
        "\n",
        "    process_btn.click(process_and_create_agent, inputs=[file_input], outputs=[status_box])\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "_HWOEgXpkpdS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}